# Low-Resolution Self-Attention (LRSA) 

## 1. 动机

### 现有方法的问题：
- **计算开销过大**：传统自注意力机制的计算复杂度为O(N²C + NC²)，其中N是token数量，当处理高分辨率图像时计算成本极高[2][3]
- **高分辨率依赖**：现有方法普遍认为语义分割需要高分辨率特征来保证准确性，导致计算瓶颈[1][2]
- **现有优化方案的局限**：
  - 窗口注意力方法（如Swin）：只在局部窗口内计算，牺牲了全局感受野[2][5]
  - 下采样方法（如SegFormer）：仅下采样key和value，query保持原分辨率，随输入分辨率增大，计算量仍然显著增加[3][5]

### 提出LRSA的目的：
- **挑战传统认知**：质疑"高分辨率对于捕捉上下文信息是必需的"这一传统观念[3][6]
- **极致降低复杂度**：在保持全局感受野和空间相关性的同时，将自注意力复杂度降至O(NC + C²)[5][7]
- **平衡效率与性能**：用低分辨率空间捕捉全局上下文，同时通过其他机制保留细节信息[3][7]

---

## 2. 模块工作原理和核心思想

### 核心思想：
**在固定的低分辨率空间中计算自注意力，无论输入图像分辨率如何**[1][3][7]

### 工作原理：

#### (1) **固定尺寸下采样**
```
输入特征 Fin ∈ R^(N×C) 
    ↓ 下采样到固定尺寸 m×m (如16×16)
下采样特征 ∈ R^(m×C)
```
- 使用2D池化操作将输入特征下采样到固定大小m（语义分割中m=16²，分类任务中m=7²）[7][10]
- 关键创新：**固定尺寸而非固定比例**，使得计算量不随输入分辨率变化[3][7]

#### (2) **低分辨率自注意力计算**
```
Attention(Fin) = Softmax(Qp·Kp^T / √dk) · Vp
```
- Qp, Kp, Vp都从下采样后的特征通过线性变换得到[7]
- **所有组件（Q、K、V）都在低分辨率空间**，这是与现有方法的最大区别[3][7]
- 计算复杂度：O(m²C)，由于m是常数，实际复杂度为O(C²)[7]

#### (3) **双线性插值上采样**
- 将自注意力输出通过双线性插值恢复到原始分辨率[7]
- 上采样操作计算量为O(NC)，延迟仅占整体网络的0.8%[19]

#### (4) **深度卷积补偿细节**
为弥补低分辨率可能损失的空间局部性，引入两处3×3深度卷积：
- **LRSA之前**：作为条件位置编码，提供空间先验[7][8]
- **FFN内部**：在两个线性层之间，捕捉高分辨率空间的细粒度细节[7][8]

```
F'in = Fin + DWConv(Fin)  // 位置编码
Fatt = F'in + LRSA(LayerNorm(F'in))  // 低分辨率全局建模
Fout = Fatt + FFN(LayerNorm(Fatt))  // FFN内含DWConv
```

#### (5) **金字塔池化增强**
在计算key和value时使用金字塔池化提取多尺度特征，以可忽略的成本增强多尺度学习能力[10]

### 复杂度对比：
| 方法 | 全局感受野 | 空间相关性 | 复杂度 |
|------|-----------|-----------|--------|
| 窗口注意力 | ✘ | ✔ | O(NC²) |
| 因子化注意力 | ✔ | ✘ | O(NC²) |
| 下采样注意力 | ✔ | ✔ | O(N²C + NC²) |
| **LRSA** | **✔** | **✔** | **O(NC + C²)** |

[5]

---

## 3. 总结

### 技术优势：
1. **极致的计算效率**：
   - 在1024×1024输入下，LRSA的FLOPs仅0.9G，而SegFormer为54G[21]
   - 随输入分辨率增大，优势更加显著（1536×1536时：1.1G vs 293.6G）[21]

2. **设计简洁有效**：
   - 固定池化尺寸16×16即可达到性能饱和[17]
   - 更大的池化尺寸（32×32、48×48）带来的性能提升微乎其微，但计算开销显著增加[17]

3. **全面的性能提升**：
   - 在ADE20K、COCO-Stuff、Cityscapes三大数据集上全面超越现有方法[12][13][14][15]
   - 对小、中、大尺寸物体均有良好表现（虽然过小的池化尺寸会影响小物体）[18]

### 核心洞察：
**LRSA证明了语义分割中的全局上下文建模不需要高分辨率**[1][3]。通过以下机制的协同：
- 低分辨率空间的全局自注意力（捕捉上下文）
- 高分辨率空间的深度卷积（保留细节）
- 多层级特征聚合（融合语义信息）

可以在大幅降低计算成本的同时，实现更优的语义分割性能[7][8][24]。这为未来视觉Transformer的设计开辟了新方向[24]。