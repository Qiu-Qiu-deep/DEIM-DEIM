# Mask Unit Attention

## 1. 动机（现有方法的问题与提出目的）

**现有方法的问题：**

- **MViTv2的池化注意力（Pooling Attention）成本高昂**：MViTv2在前两个阶段使用池化注意力机制，对K和V进行池化以减少注意力矩阵的计算量。但对于大输入（特别是视频数据），这种全局注意力仍然计算开销很大 [7][9]。

- **稀疏MAE预训练的兼容性问题**：MAE预训练采用稀疏策略（删除被遮罩的token而非覆盖），这会破坏多阶段模型依赖的2D网格结构。传统的窗口注意力（如Swin Transformer）在下采样后会与已删除的token重叠，导致兼容性问题 [5][9]。

**提出目的：**

- 在保持稀疏MAE预训练效率的前提下，用一种**实现简单、无额外开销**的局部注意力机制替代池化注意力
- 解决局部注意力与稀疏预训练的兼容性问题，避免窗口与已删除token的重叠 [9]

## 2. 模块工作原理和核心思想

**核心思想：**

Mask Unit Attention的关键创新是**将注意力的局部窗口与MAE预训练中的mask unit对齐**，而非使用固定大小的窗口 [9]。

**工作原理：**

1. **Mask Unit的定义**：在Hiera中，mask unit是应用MAE遮罩的基本单元（32×32像素区域），而token是模型内部的分辨率单元（初始为4×4像素）。一个mask unit在不同阶段对应不同数量的token（第1阶段：8×8个token，第2阶段：4×4个token，第3阶段：2×2个token，第4阶段：1×1个token）[5][6]。

2. **零开销的实现**：由于MAE预训练时已经在网络开始处将token按mask unit分组，这些token在到达注意力层时已经自然地按单元组织好了。因此可以**直接在每个mask unit内部执行局部注意力，无需额外的分组操作** [9]。

3. **自适应窗口大小**：与固定窗口注意力不同，Mask Unit Attention的窗口大小会**随着网络阶段自动调整**以适应当前分辨率的mask unit大小，避免了窗口泄漏到已删除token的问题 [9]。

4. **应用范围**：仅在前两个阶段（特征分辨率较高时）使用Mask Unit Attention进行局部注意力；在第3、4阶段使用全局注意力，因为此时全局注意力的计算成本已经可以接受 [7][9]。

**与其他方法的对比：**

- **vs. 池化注意力（Pooling Attention）**：不需要对K和V进行池化操作，直接在局部执行注意力，更高效 [7]
- **vs. 窗口注意力（Window Attention）**：窗口大小固定，会在下采样后与已删除token重叠；而Mask Unit Attention的窗口大小自适应，完美兼容稀疏预训练 [9]
- **vs. Swin的移位窗口**：不需要窗口移位机制，因为后续阶段使用全局注意力 [7]

## 3. 总结

Mask Unit Attention是一个**极简且高效**的局部注意力机制，通过将注意力窗口与MAE预训练的mask unit对齐，实现了三个关键优势：

1. **零实现开销**：利用已有的mask unit分组，无需额外操作
2. **完美兼容稀疏预训练**：自适应窗口避免了与已删除token的冲突
3. **显著的性能提升**：在保持精度不变的情况下，使模型吞吐量提升高达32%（视频任务）[7][9]

这个模块体现了Hiera的设计哲学：**通过巧妙的设计而非复杂的机制来实现效率和准确性的统一**。它成功替代了MViTv2中复杂的池化注意力，成为Hiera简化架构过程中的关键创新之一 [7][8][9]。