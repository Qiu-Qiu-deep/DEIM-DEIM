\section{Related Work}

% \textbf{Real-Time Object Detectors.} The YOLO series has been at the forefront of real-time object detection, continuously evolving through innovations in architecture, data augmentation, and training techniques~\citep{redmon2016you, NEURIPS2023_a0673542, redmon2018yolov3,bochkovskiy2020yolov4,yolov5v7.0,li2023yolov6v3,ge2021yolox,wang2023yolov7,yolov8, wang2024yolov9}. These models, while efficient, typically rely on Non-Maximum Suppression (NMS) for post-processing, which introduces additional latency and complicates the balance between speed and accuracy. Moreover, the YOLO series benefits from a robust and active community, contributing to its widespread adoption and continuous improvement across various applications.


% \textbf{End-to-End Object Detectors.} End-to-end object detection models, introduced by DETR~\citep{carion2020end}, have revolutionized the field by eliminating the need for hand-crafted components like anchors and NMS. Traditional DETR models~\citep{zhu2020deformable, meng2021conditional, zhang2022dino, wang2022anchor, liu2021dab, li2022dn, chen2022groupv2, chen2022group}, have achieved state-of-the-art results, particularly excelling in accuracy. Despite their success, these models are not suitable for real-time applications due to their high computational costs and latency. Recent advancements, including RT-DETR~\citep{lv2023detrs} and LW-DETR~\citep{chen2024lw}, have adapted the DETR framework to achieve real-time performance, offering a balance between speed and accuracy. The latest introduced YOLOv10~\citep{wang2024yolov10} eliminates the need for NMS. This positions YOLOv10 uniquely within the series, aligning real-time performance with the benefits of end-to-end detection.


\textbf{Real-Time / End-to-End Object Detectors.} The YOLO series has led the way in real-time object detection, evolving through innovations in architecture, data augmentation, and training techniques~\citep{redmon2016you, NEURIPS2023_a0673542, wang2023yolov7, yolov8, wang2024yolov9, yolo11}. While efficient, YOLOs typically rely on Non-Maximum Suppression (NMS), which introduces latency and instability between speed and accuracy. DETR~\citep{carion2020end} revolutionizes object detection by removing the need for hand-crafted components like NMS and anchors. Traditional DETRs~\citep{zhu2020deformable, meng2021conditional, zhang2022dino, wang2022anchor, liu2021dab, li2022dn, chen2022group, chen2022groupv2} have achieved excelling performance but at the cost of high computational demands, making them unsuitable for real-time applications. Recently, RT-DETR~\citep{lv2023detrs} and LW-DETR~\citep{chen2024lw} have successfully adapted DETR for real-time use. Concurrently, YOLOv10~\citep{wang2024yolov10} also eliminates the need for NMS, marking a significant shift towards end-to-end detection within the YOLO series. 


\textbf{Distribution-Based Object Detection.} 
Traditional bounding box regression methods~\citep{yolov1,SSD,fasterrcnn} rely on Dirac delta distributions, treating bounding box edges as precise and fixed, which makes modeling localization uncertainty challenging. To address this, recent models have employed Gaussian or discrete distributions to represent bounding boxes~\citep{gaussian_yolov3,gfocal,offsetbin, gfocalv2}, enhancing the modeling of uncertainty. However, these methods all rely on anchor-based frameworks, which limits their compatibility with modern anchor-free detectors like YOLOX~\citep{ge2021yolox} and DETR~\citep{carion2020end}. Furthermore, their distribution representations are often formulated in a coarse-grained manner and lack effective refinement, hindering their ability to achieve more accurate predictions.


\textbf{Knowledge Distillation.} Knowledge distillation (KD) \citep{hinton2015distilling} is a powerful model compression technique. Traditional KD typically focuses on transferring knowledge through Logit Mimicking~\citep{Zagoruyko2017AT,TA,DenselyTA}. FitNets~\citep{FitNets} initially propose Feature Imitation, which has inspired a series of subsequent works that further expand upon this idea~\citep{chen2017learning, GIbox, defeat, Li_2017_CVPR, wang2019distilling}. Most approaches for DETR~\citep{chang2023detrdistill, wang2024kd} incorporate hybrid distillations of both logit and various intermediate representations. Recently, localization distillation (LD)~\citep{zheng2022localization} demonstrates that transferring localization knowledge is more effective for detection tasks. Self-distillation~\citep{zhang2019your, zhang2021self} is a special case of KD which enables earlier layers to learn from the model's own refined outputs, requiring far fewer additional training costs since there's no need to separately train a teacher model.
