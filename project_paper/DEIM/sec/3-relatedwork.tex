

\section{Related Work} 
\label{sec:relate}

\textbf{Object detection with transformer (DETR)}~\cite{carion2020end} represents a shift from traditional CNN architectures to transformers. By using Hungarian~\cite{kuhn1955hungarian} algorithm for one-to-one matching, DETR eliminates the need for hand-crafted NMS as the post-processing and enables end-to-end object detection. However, it suffers from slow convergence and dense computation.


\paragraph{Increasing positive samples.}
One-to-one matching limits each target to a single positive sample, providing far less supervision than O2M and hindering the optimization. Some studies have explored ways to increase supervision within the O2O framework. Group DETR~\cite{chen2023group}, for instance, employs the concept of “groups” to approximate the O2M. It uses $K$ groups of queries, where $K > 1$, and performs O2O matching independently within each group. This allows each target to be assigned $K$ positive samples. However, to prevent communication between groups, each group requires a separate decoder layer, ultimately resulting in $K$ parallel decoders. The hybrid matching scheme in H-DETR~\cite{jia2023detrs} works similarly to Group DETR. Co-DETR~\cite{zong2023detrs} reveals that a one-to-many assignment approach helps the model learn more distinctive feature information, so it proposed a collaborative hybrid assignment scheme to enhance encoder representations through auxiliary heads with one-to-many label assignments, like Faster R-CNN~\cite{ren2016faster} and FCOS~\cite{tian2022fully}. The existing methods aim to increase the number of positive samples per target to enhance supervision. In contrast, Our Dense O2O explores another direction — increasing the number of targets per training image to boost supervision effectively. Unlike existing methods, which require additional decoders or heads and thus increase training resource consumption, our approach is computation-free.


\paragraph{Optimizing low-quality matches.} 

The sparse and randomly initialized queries lack spatial alignment with targets, resulting in a high proportion of low-quality matches that impede model convergence. Several methods have introduced prior knowledge into query initialization, such as anchor queries~\cite{wang2022anchor}, DAB-DETR~\cite{liu2022dab}, DN-DETR~\cite{li2022dn}, and dense distinct queries~\cite{zhang2023dense}. More recently, inspired by two-stage paradigms~\cite{ren2016faster, zhu2020deformable}, methods like DINO~\cite{zhang2022dino} and RT-DETR~\cite{zhao2024detrs} leverage top-ranked predictions from the encoder's dense outputs to refine decoder queries~\cite{yao2021efficient}. These strategies enable more effective query initialization closer to target regions. However, low-quality matches persist as a significant challenge~\cite{liu2023detection}. In RT-DETR~\cite{zhao2024detrs}, Varifocal Loss (VFL) is employed to reduce the uncertainty between classification confidence and box quality, enhancing real-time performance. Yet, VFL is primarily designed for traditional detectors with fewer low-quality matches and focuses on high-IoU optimization, leaving low-IoU matches under-optimized due to their minimal and flat loss values. Building on those advanced initializations, we introduce a matchability-aware loss to better optimize matches across varying quality levels, significantly enhancing the effectiveness of Dense O2O matching.

\paragraph{Reducing computation cost.}

Standard attention mechanisms involve dense computation. To improve efficiency and facilitate interactions with multi-scale features, several advanced attentions have been developed, such as deformable attention~\cite{zhu2020deformable}, multi-scale deformable attention~\cite{zhao2024ms}, dynamic attention~\cite{dai2021dynamic}, and cascade window attention~\cite{ye2023cascade}. Additionally, recent research has focused on creating more efficient encoders. For example, Lite DETR~\cite{li2023lite} introduces an encoder block that interleaves updates between high-level and low-level features, while RT-DETR~\cite{zhao2024detrs} combines CNN and self-attention in its encoder. Both designs significantly reduce resource consumption, especially RT-DETR. RT-DETR is the first real-time object detection model within the DETR framework. Building on this hybrid encoder, D-FINE~\cite{peng2024d} further optimizes RT-DETR with additional modules and refines the regression process by iteratively updating probability distributions instead of predicting fixed coordinates. This approach enables D-FINE to achieve a more favorable trade-off between latency and performance, slightly surpassing recent YOLO models. Leveraging these advancements in real-time DETRs, our method achieves impressive performance with reduced training costs, outperforming YOLO models by a substantial margin in real-time object detection.



