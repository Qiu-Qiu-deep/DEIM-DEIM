\section{Experiments}

\subsection{Datasets}
\hspace{\parindent}To demonstrate the effectiveness of our model, we conduct experiments on the aerial dataset AI-TOD-V2~\cite{AITODv2}, which mostly consists of tiny objects.

\noindent \textbf{AI-TOD-V2.}
This dataset includes 28,036 aerial images with 752,745 annotated object instances. 
There are 11,214 images for the train set, 2,804 for the validation set, and 14,018 for the test set.
The average object size in AI-TOD-V2 is only 12.7 pixels, with 86\% of objects in the dataset smaller than 16 pixels, and even the largest object is no bigger than 64 pixels. 
Also, the number of objects in an image can vary enormously from 1 to 2667, where the average number of objects per image is 24.64, with a standard deviation of 63.94.

\noindent \textbf{VisDrone.}
This dataset includes 14,018 drone-shot images, with 6,471 images for the train set, 548 for the validation set, and 3,190 for the test set.
There are 10 categories, and the image resolution is 2000 $\times$ 1500 pixels.
Also, the images are diverse in a wide range of aspects, including objects (pedestrians, vehicles, bicycles, etc.) and density (sparse and crowded scenes), where the average number of objects per image is 40.7 with a standard deviation of 46.41.

\noindent \textbf{Evaluation Metric.}
We use the AP (Average Precision) metric with a max detection number of 1500 to evaluate the performance of our proposed method. 
Specifically, AP means the average value from $\text{AP}_{50}$ to $\text{AP}_{95}$ , with IoU interval of 0.05.
Moreover, $\text{AP}_{vt}$, $\text{AP}_{t}$, $\text{AP}_{s}$, and $\text{AP}_{m}$ are for very tiny, tiny, small, and medium scale evaluation in AI-TOD~\cite{AI-TOD}. 

%-------------------------------------------------------------------------
\subsection{Implementation Details}
\hspace{\parindent}Based on the DETR-like structure, we use a 6-layer transformer encoder, a 6-layer transformer decoder with 256 as the hidden dimension, and a ResNet50 as our CNN backbone.
Furthermore, we train our model with Adam optimizer using NVIDIA 3090 GPUs. 
The batch size is set to 1 due to memory constraints.
The same random crop and scale augmentation strategies are applied following DETR~\cite{DETR}.
In addition, to minimize errors that propagate from the categorical counting module to dynamic query selection, we apply a two-stage training scheme.
We first train the categorical counting module to achieve more stable results for the number of queries in the transformer decoder.
After stabilizing the counting result, we add the counting-guided feature enhancement module into training to refine the encoder's visual features with density maps.

%\usepackage{tabularray}
\begin{table*}[ht]
\caption{\textbf{Experiments on AI-TOD-V2.} All models are trained on the \textit{trainval} split and evaluated on the \textit{test} split. * denotes a re-implementation of the results.}
\centering
% \scriptsize
\resizebox{0.98\linewidth}{!}{
\begin{tblr}{
  row{2} = {c},
  row{11} = {c},
  column{even} = {c},
  column{3} = {c},
  column{5} = {c},
  column{7} = {c},
  column{9} = {c},
  cell{2}{1} = {c=9}{},
  cell{11}{1} = {c=9}{},
  hline{1-3,11-12,16-17} = {-}{},
}
Method                    & Backbone      & AP   & $\text{AP}_{50}$ & $\text{AP}_{75}$ & $\text{AP}_{vt}$ & $\text{AP}_{t}$ & $\text{AP}_{s}$ & $\text{AP}_{m}$ \\
\hline
\qquad \qquad CNN-based models    &                   &               &      &      &      &        &       &       &       \\
YOLOv3~\cite{yolov3}                        & Darknet53     & 4.1  & 14.6 & 0.9  & 1.1    & 4.8   & 7.7   & 8.0      \\
RetinaNet~\cite{focal_loss}                       & ResNet50-FPN & 8.9  & 24.2 & 4.6  & 2.7    & 8.4   & 13.1  & 20.2   \\
Faster-RCNN~\cite{frcnn}                     & ResNet50-FPN & 12.8 & 29.9 & 9.4  & 0.0      & 9.2   & 24.6  & 37.0     \\
Cascade R-CNN~\cite{crcnn}                     & ResNet50-FPN & 15.1 & 34.2 & 11.2 & 0.1    & 11.5  & 26.7  & 38.5  \\
DetectoRS~\cite{detectors}                      & ResNet50-FPN & 16.1 & 35.5 & 12.5 & 0.1    & 12.6  & 28.3  & 40.0  \\
DotD~\cite{dot}                            & ResNet50-FPN & 20.4 & 51.4 & 12.3 & 8.5    & 21.1  & 24.6  & 30.4  \\
NWD-RKA~\cite{AITODv2}                        & ResNet50-FPN & 24.7 & 57.4 & 17.1 & 9.7    & 24.2  & 29.8  & 39.3  \\
RFLA~\cite{rfla}                             & ResNet50-FPN & 25.7 & 58.9 & 18.8 & 9.2    & 25.5  & 30.2  & 40.2  \\
\hline
\qquad \qquad DETR-like models   &  &                &               &      &      &      &        &       &       &       \\
DETR-DC5*~\cite{DETR}                       & ResNet50     & 10.4 & 32.5 & 3.9  & 3.6    & 9.3   & 13.2  & 24.6  \\
Deformable-DETR*~\cite{Deformable-DETR}                   & ResNet50     & 18.9 & 50.0 & 10.5 & 6.5    & 17.6  & 25.3  & 34.4  \\
DAB-DETR*~\cite{DAB-DETR}                          & ResNet50     & 22.4 & 55.6 & 14.3 & 9.0    & 21.7  & 28.3  & 38.7  \\
DINO-DETR*~\cite{DINO}                         & ResNet50     & 25.9 & 61.3 & 17.5 & 12.7   & 25.3  & 32.0  & 39.7  \\
\textbf{DQ-DETR (Ours)}                   & ResNet50     & \textbf{30.2 (+4.3)} & \textbf{68.6} & \textbf{22.3} & \textbf{15.3}   & \textbf{30.5}  & \textbf{36.5}  & \textbf{44.6}  
\end{tblr}
}
\label{table:Overall}
\end{table*}

% \vspace{-10pt}
%-------------------------------------------------------------------------
\subsection{Main Results}
\noindent \textbf{AI-TOD-V2.}
Table \ref{table:Overall} shows our main results on the AI-TOD-V2 test split. 
We compare the performances of our DQ-DETR with strong baselines, including both CNN-based and DETR-like methods. 
All CNN-based methods except YOLOv3 use ResNet50 with feature pyramid network (FPN)~\cite{FPN}.
Moreover, since there is no previous research on DETR-like models for tiny object detection, our DQ-DETR is the first DETR-like model that focuses on detecting tiny objects.
We re-implement a series of DETR-like models on AI-TOD-V2, and all DETR-like methods except DETR use 5-scale feature maps with deformable attention~\cite{Deformable-DETR}. 
For 5-scale feature maps, features are extracted from stages 1, 2, 3, and 4 of the backbone, and add the extra feature by down-sampling the output of stage 4.  

The results are summarized in Table \ref{table:Overall}, our proposed DQ-DETR achieves the best result \textbf{30.2} AP compared with other state-of-the-art methods, including CNN-based and DETR-like methods. 
Also, DQ-DETR surpasses the baseline by 20.5\%, 20.6\%, 14.1\%, and 12.3\% in terms of $\text{AP}_{vt}$, $\text{AP}_{t}$, $\text{AP}_{s}$, $\text{AP}_{m}$. 
The performance gain is greater on $\text{AP}_{vt}$, and $\text{AP}_{t}$, and our DQ-DETR outperforms the advanced series of DETR-like models on AI-TOD-V2. We credit the performance gains for the following reasons: (1) DQ-DETR fuses the transformer visual features with a density map from the categorical counting module to improve the positional information of object queries, which makes the queries more suitable for localizing tiny objects. (2) Our dynamic query selection adaptively chooses an adequate number of object queries used for the detection task and can handle the images with either few or crowded objects.

% \usepackage{tabularray}
\begin{table}[h!]
\caption{\textbf{Experiments on VisDrone.} All models are trained on the \textit{train} split and evaluated on the \textit{val} split. * denotes a re-implementation of the results.}
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[438]Q[131]Q[158]Q[158]},
  column{even} = {c},
  column{3} = {c},
  vline{2} = {-}{},
  hline{1-2,9-10} = {-}{},
}
Model                        & AP   & $\text{AP}_{50}$ & $\text{AP}_{75}$ \\
Faster R-CNN~\cite{frcnn}    & 21.4 & 40.7 & 19.9 \\
Cascade R-CNN~\cite{crcnn}   & 22.6 & 38.8 & 23.2 \\
Yolov5~\cite{yolov5}         & 24.1 & 44.1 & 22.3 \\
CEASC~\cite{CEASC}           & 28.7 & 50.7 & 24.7 \\
SDP~\cite{SDP}               & 30.2 & 52.5 & 28.4 \\
DNTR~\cite{dntr}            & 33.1 & 53.8 & 34.8 \\ 
DINO-DETR*~\cite{DINO}       & 35.8 & 58.3 & 36.8 \\
\textbf{DQ-DETR (Ours)}               & \textbf{37.0} & \textbf{60.9} & \textbf{37.9} 
\end{tblr}
}
\label{table:visdrone}
\end{table}
% \vspace{-15pt}
%-------------------------------------------------------------------------

% \subsection{Results on VisDrone}
\noindent \textbf{VisDrone.}
We also conduct experiments on the VisDrone~\cite{visdrone} dataset to demonstrate the effectiveness of our model DQ-DETR.
Table \ref{table:visdrone} shows our results on the VisDrone \textit{val} split. 
We compare the performances of our DQ-DETR with other methods.
Our proposed DQ-DETR achieves the best result \textbf{37.0} AP compared with other state-of-the-art methods, including CNN-based and DETR-like methods. 
Also, DQ-DETR surpasses the baseline DINO-DETR by 1.2, 2.6, and 1.1 in terms of AP, $\text{AP}_{50}$, $\text{AP}_{75}$. 

% \subsection{Results on COCO}
\noindent \textbf{COCO.}
\hspace{\parindent}We compare our method, DQ-DETR, with the previous state-of-the-art on the COCO dataset. DQ-DETR yielded slightly lower performance, with an AP of 50.2 compared to 51.3. We believe several factors contributed to these results. Firstly, our experiments were constrained by limited GPU resources, which may have impacted our ability to optimize the training process. Secondly, our method is specifically designed for tiny object detection in scenarios where the number of objects varies significantly across images. This specialized focus may not be fully leveraged in the COCO dataset, which is a general object detection task with a nearly balanced number of objects per image. Therefore, while our method shows potential, it may not perform as expected on general datasets like COCO due to various factors.

% \usepackage{tabularray}
\begin{table}[h!]
\centering
\scriptsize
\caption{\textbf{Experiments on COCO.} All models are trained on the \textit{train} split and evaluated on the \textit{val} split.}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[295]Q[115]Q[79]Q[96]Q[96]Q[79]Q[83]Q[79]},
  column{even} = {c},
  column{3} = {c},
  column{5} = {c},
  column{7} = {c},
  vline{2-3} = {-}{},
  hline{1-2,8} = {-}{},
}
Method                                  & Epochs & AP & $\text{AP}_{50}$ & $\text{AP}_{75}$   & $\text{AP}_{S}$  & $\text{AP}_{M}$ & $\text{AP}_{L}$  \\
Faster R-CNN~\cite{frcnn}               & 109    & 42.0 & 62.1 & 45.5 & 26.6 & 45.5 & 53.4 \\
DETR~\cite{DETR}                        & 500    & 43.3 & 63.1 & 45.9 & 22.5 & 47.3 & 61.1 \\
Deformable DETR~\cite{Deformable-DETR}  & 50     & 46.2 & 65.2 & 50.0 & 28.8 & 49.2 & 61.7 \\
DN-DETR~\cite{DN-DETR}                  & 50     & 46.3 & 66.4 & 49.7 & 26.7 & 50.0 & 64.4 \\
DINO-DETR~\cite{DINO}                   & 24     & 51.3 & 69.1 & 56.0 & 34.5 & 54.2 & 65.8 \\
DQ-DETR                                 & 24     & 50.2 & 67.1 & 55.0 & 31.9 & 53.2 & 64.7 
\end{tblr}
\label{exp:COCO}
\end{table}
\vspace{-20pt}
%-------------------------------------------------------------------------
\subsection{Ablation Study}
\hspace{\parindent}Categorical counting module, counting-guided feature enhancement, and dynamic query selection are the newly proposed contributions.
We conduct a series of ablation studies to verify the effectiveness of each component proposed in this paper.
DINO-DETR is chosen as the comparing DETR-like baseline.

\subsubsection{Main ablation experiment.}
\hspace{\parindent}Table \ref{ablation:Diff_module} shows the performance of our contributions separately on AI-TOD-V2. 
The results demonstrate that each component in DQ-DETR contributes to performance improvement. 
We attain an improved +2.2 AP over the baseline with the categorical counting module and dynamic query selection.
Furthermore, with feature enhancement refining the encoder's feature, it gains an extra improvement of +4.3, +2.6, +5.2 on AP, $\text{AP}_{vt}$, and $\text{AP}_{t}$ over the baseline.
Besides, the experiment with the counting module and feature enhancement together but without dynamic query selection further shows that introducing an additional counting-guided feature-enhancing task improves performance, even when query numbers remain static.
Consequently, we prove the power of each component in DQ-DETR on AI-TOD-V2.

% \usepackage{tabularray}
\begin{table*}[h!]
\caption{Overall ablation for our architecture on AI-TOD-V2 \textit{test} split. Note that CC, DQS, and FE represent categorical counting, dynamic query selection, and feature enhancement, respectively.}
\centering
\resizebox{0.85\linewidth}{!}{
\scriptsize
\begin{tblr}{
  cells = {c},
  hline{1-2,6} = {-}{},
  vline{2-4} = {-}{},
}
{CC} & {DQS} & {FE} &AP   & $\text{AP}_{vt}$ & $\text{AP}_{t}$ & $\text{AP}_{s}$ & $\text{AP}_{m}$ \\
        &                          &                            &  25.9 & 12.7   & 25.3  & 32.0  & 39.7  \\
\checkmark & \checkmark                       &                 & 28.1 & 12.3   & 27.8  & 34.6  & 44.1  \\
\checkmark &                      &   \checkmark                & 29.1 & 14.4   & 29.3  & 35.2  & 44.1  \\
\checkmark & \checkmark        & \checkmark                     & \textbf{30.2} & \textbf{15.3}   & \textbf{30.5}  & \textbf{36.5}  & \textbf{44.6}  
\end{tblr}
}
\label{ablation:Diff_module}
\end{table*}
\vspace{-20pt}

\subsubsection{Ablation of DQ-DETR with different number of instances in images.}
\hspace{\parindent}We explore our DQ-DETR's performance under different numbers of instances in an image. 
We classify the AI-TOD-V2 dataset into 4 levels based on the number of instances $N$ in the image as in the categorical counting module, i.e., $N \leq 10$, $10 < N \leq 100$, $100 < N \leq 500$, and $500 < N$.
Our proposed DQ-DETR's performance is analyzed under these four situations. The results are shown in Table \ref{ablation:Diff_cls} compared with DINO-DETR as the baseline.
Our DQ-DETR dynamically adjusts the number of object queries based on the number of instances in the image, while DINO-DETR always uses 900 queries in all situations.

We can observe that in the situations of $N \leq 10$, and $10 < N \leq 100$, our DQ-DETR uses fewer numbers of queries and outperforms the baseline by 16\%, and 16.4\% in terms of AP. The performances in terms of $\text{AP}_{vt}$, $\text{AP}_{t}$ surpass the baseline by 19.8\%, and 20.8\% as well.
Moreover, it is noteworthy that DINO-DETR performs poorly when $N > 500$. Under these circumstances, there might be over 900 instances in some of the images, which is beyond the detection capability of DINO-DETR.
In dense images, the detection limitation of DINO-DETR with only 900 queries, leads to many objects undetected (FN), resulting in a lower AP.
Our DQ-DETR dynamically selects more queries for dense images, remarkably surpassing the baseline by 42.1\% in terms of $\text{AP}_{vt}$.

% \usepackage{tabularray}
\begin{table*}[h!]
\caption{\textbf{Evaluation results for varying instance counts.} N indicates the number of instances in the image. We separate the AI-TOD-V2 dataset into 4 classes based on N. All models are trained on the AI-TOD-V2 \textit{trainval} split and evaluated on \textit{test} split.}
\centering
% \scriptsize
\resizebox{0.98\linewidth}{!}{
\begin{tblr}{
  column{3} = {c},
  column{4} = {c},
  column{5} = {c},
  column{6} = {c},
  column{7} = {c},
  column{8} = {c},
  column{9} = {c},
  column{10} = {c},
  vline{2-4} = {-}{},
  hline{1-2,7,12} = {-}{},
  %hline{6} = {2-10}{}
}
Model       & \#Objects in image         & \#Query & AP   & $\text{AP}_{50}$ & $\text{AP}_{75}$ & $\text{AP}_{vt}$ & $\text{AP}_{t}$ & $\text{AP}_{s}$ & $\text{AP}_{m}$ \\
            & N $\leq 10$                 & 900     & 22.5 & 53.1 & 14.8 & 10.6   & 24.5  & 25.7  & 34.9  \\
            & $10 <$ N $\leq 100$         & 900     & 24.4 & 58.8 & 15.9 & 13.0   & 22.9  & 31.5  & 37.3  \\
DINO-DETR~\cite{DINO}   & $100 <$ N $\leq 500$        & 900     & 31.6 & 67.3 & 26.9 & 10.1   & 25.4  & 39.6  & 38.2  \\
            & $500 <$ N                & 900     & 13.5 & 27.9 & 7.3  & 5.7    & 6.4   & 34.7  & 32.4  \\
            & Overall                   & 900     & 25.9 & 61.3 & 17.5 & 12.7   & 25.3  & 32.0  & 39.7   \\
\hline
            & N $\leq 10$                 & 300     & 26.1 & 60.4 & 19.7 & 12.7   & 29.6  & 28.5  & 40.8  \\
            & $10 <$ N $\leq 100$         & 500     & 28.4 & 65.9 & 20.1 & 15.2   & 27.8  & 34.7  & 41.8  \\
DQ-DETR (Ours)   & $100 <$ N $\leq 500$    & 900     & 33.7 & 69.9 & 30.4 & 11.1   & 30.4  & 42.0  & 41.6  \\
            & $500 <$ N                & 1500    & 14.7 & 35.6 & 7.5  & 8.1    & 7.8   & 37.5  & 40.4  \\
            & Overall                   & Dynamic & 30.2 & 68.6 & 22.3 & 15.3   & 30.5  & 36.5  & 44.6
\end{tblr}
}
\label{ablation:Diff_cls}
\end{table*}
% \vspace{-15pt}
\subsubsection{Ablation of Categorical Counting Module.}
\hspace{\parindent}Table \ref{ablation:cls_acc} demonstrates the accuracy of the classification task in our categorical counting module. 
The performance is analyzed under four situations, where $N$ is the number of instances per image.
The total classification accuracy is about 94.6\%, which means our categorical counting module can accurately estimate the number of objects $N$ in the images.
Furthermore, we can find that our categorical counting module has a poor classification performance with only 56.6\% accuracy in the $N > 500$ situation since the number of training images is much fewer in this situation. 
Also, we observe that there are at most 2267 instances per image in the AI-TOD-V2 dataset. However, the long-tailed distribution of the training samples restricts us from classifying the number of instances $N$ in more detail. We have no choice but to categorize the images with $500 < N \leq 2267$ into the same class.

As for the detection accuracy, our DQ-DETR outperforms the baseline under all situations. 
The performances surpass the baseline by 16\% and 16.4\% in terms of AP for $N \leq 10$, $10 < N \leq 100$.
Nevertheless, our DQ-DETR performs slightly better than the baseline in the scenario with $N > 500$.  
This phenomenon is due to the poor classification accuracy for $N > 500$.
The incorrect prediction from the categorical counting module directly affects the number of object queries used for detection, where the inappropriate number of queries might harm the detection performance.
% The incorrect prediction from the categorical counting module will directly affects the number of object queries used for detection, where the detection performance might be harmed by the inappropriate number of queries.

% \usepackage{tabularray}
\begin{table*}[h!]
\caption{The classification accuracy of our categorical counting module and detection accuracy of DQ-DETR with different numbers of instances in the images. DINO-DETR is compared as the baseline.}
\centering
\resizebox{0.98\linewidth}{!}{
\begin{tblr}{
  column{even} = {c},
  column{3} = {c},
  column{5} = {c},
  vline{2-5} = {-}{},
  hline{1-2,6-7} = {-}{},
}
\#Objects in image                & Accuracy(\%) & AP (DQ-DETR) & AP (Baseline) & \#Sample \\
N $\leq 10$           & 97.7         & 26.1 (+3.6)      & 22.5          & 8674     \\
$10 <$ N $\leq 100$   & 90.5         & 28.4 (+4.0)      & 24.4          & 4393     \\
$100 <$ N $\leq 500$  & 86.5         & 33.7 (+2.1)      & 31.6          & 905      \\
$500 <$ N          & 56.5         & 14.7 (+1.2)      & 13.5          & 46       \\
Total               & 94.6         & 30.2             & 25.9          & 14018    
\end{tblr}
}
\label{ablation:cls_acc}
\end{table*}

% \usepackage{tabularray}
\begin{table}[h!]
\caption{Ablation of using regression or classification in categorical counting module.}
\centering
% \scriptsize
\begin{tblr}{
  column{2} = {c},
  column{3} = {c},
  column{4} = {c},
  column{5} = {c},
  column{6} = {c},
  vline{2} = {-}{},
  hline{1-2,5} = {-}{},
  % hline{4-5,6} = {-}{},
}
Method         & AP   & $\text{AP}_{vt}$ & $\text{AP}_{t}$ & $\text{AP}_{s}$ & $\text{AP}_{m}$ \\
Baseline       & 25.9 & 12.7   & 25.3  & 32.0  & 39.7  \\
Regression     & 14.9 & 5.2    & 16.3  & 19.9  & 14.3  \\
Classification & 30.2 & 15.3   & 30.5  & 36.5  & 44.6  
\end{tblr}
\label{ablation:Cls_Reg}
\end{table}

Table \ref{ablation:Cls_Reg} compares our DQ-DETR's performance of using classification or regression in the categorical counting module.
The traditional crowd-counting methods usually regress the predicted counting number to a specific value.
However, in our study, we use a classification head instead. This experiment demonstrates our DQ-DETR performance with these two methods. 
For the classification task, we classify the images into 4 classes and apply different numbers of queries in the transformer decoder, as we mentioned in the previous section.
For the regression task, we regress an integer directly to predict the number of objects in the image and select the object queries corresponding to the predicted result.

The results demonstrate that using regression as a counting method performs extremely poorly.
We impute the drastic performance drop for the following reasons: (1) It is challenging to regress an accurate number since the number of instances per image may vary significantly from 1 to 2267 in the AI-TOD-V2 dataset. (2) Unstable regression results significantly affect the number of queries used in the transformer decoder, making it difficult for the DETR model to converge. Owing to the above reasons, we believe that classifying how many objects exist in the image into different levels is a simpler way in contrast to regression. Thus, classification instead of regression is preferred as a method in our proposed categorical counting module.

